import { Researcher } from "./researchers-data"

export const researchers: Researcher[] = [
  {
    id: 13,
    slug: "megan-jenkins",
    name: "Dr. Megan Jenkins",
    avatar: "/portraits/imgs12.jpg",
    affiliation: "Stanford HAI",
    communities: ["lesswrong"],
    topics: ["AI Alignment", "AI Governance", "RLHF", "Interpretability"],
    papers: 125,
    posts: 14,
    hIndex: 11,
    bio: "Researcher at Google DeepMind focusing on ai alignment and ai governance.",
    fullBio: "Dr. Megan Jenkins is a researcher specializing in ai alignment, ai governance, rlhf, interpretability. They have published extensively in top-tier venues and are actively involved in the lesswrong community.",
    website: "https://megan-jenkins.com",
    twitter: "meganjenkins",
    publications: [],
    recentPosts: [],
    collaborators: [],
  },
  {
    id: 14,
    slug: "joan-rodriguez",
    name: "Dr. Joan Rodriguez",
    avatar: "/portraits/imgs37.jpg",
    affiliation: "Carnegie Mellon University",
    communities: ["ea", "academic", "lesswrong"],
    topics: ["Decision Theory", "AI Risk"],
    papers: 30,
    posts: 26,
    hIndex: 38,
    bio: "Researcher at Allen Institute for AI focusing on decision theory and ai risk.",
    fullBio: "Dr. Joan Rodriguez is a researcher specializing in decision theory, ai risk. They have published extensively in top-tier venues and are actively involved in the ea community.",
    website: "https://joan-rodriguez.com",
    twitter: "joanrodriguez",
    publications: [],
    recentPosts: [],
    collaborators: [],
  },
  {
    id: 15,
    slug: "daniel-brooks",
    name: "Dr. Daniel Brooks",
    avatar: "/portraits/imgs12.jpg",
    affiliation: "Allen Institute for AI",
    communities: ["academic", "ea"],
    topics: ["AI Alignment", "Value Alignment", "AI Governance", "AI Risk"],
    papers: 37,
    posts: 16,
    hIndex: 19,
    bio: "Researcher at Google DeepMind focusing on ai alignment and value alignment.",
    fullBio: "Dr. Daniel Brooks is a researcher specializing in ai alignment, value alignment, ai governance, ai risk. They have published extensively in top-tier venues and are actively involved in the academic community.",
    website: "https://daniel-brooks.com",
    twitter: "danielbrooks",
    publications: [],
    recentPosts: [],
    collaborators: [],
  },
  {
    id: 16,
    slug: "roger-roberts",
    name: "Dr. Roger Roberts",
    avatar: "/portraits/imgs34.jpg",
    affiliation: "Oxford FHI",
    communities: ["ea"],
    topics: ["Interpretability", "Mechanistic Interpretability"],
    papers: 26,
    posts: 40,
    hIndex: 14,
    bio: "Researcher at Anthropic focusing on interpretability and mechanistic interpretability.",
    fullBio: "Dr. Roger Roberts is a researcher specializing in interpretability, mechanistic interpretability. They have published extensively in top-tier venues and are actively involved in the ea community.",
    website: "https://roger-roberts.com",
    twitter: "rogerroberts",
    publications: [],
    recentPosts: [],
    collaborators: [],
  },
  {
    id: 17,
    slug: "deborah-diaz",
    name: "Dr. Deborah Diaz",
    avatar: "/portraits/imgs21.jpg",
    affiliation: "Google DeepMind",
    communities: ["academic", "lesswrong"],
    topics: ["Value Alignment", "Mechanistic Interpretability", "AI Alignment", "Deep Learning"],
    papers: 48,
    posts: 21,
    hIndex: 35,
    bio: "Researcher at UC Berkeley focusing on value alignment and mechanistic interpretability.",
    fullBio: "Dr. Deborah Diaz is a researcher specializing in value alignment, mechanistic interpretability, ai alignment, deep learning. They have published extensively in top-tier venues and are actively involved in the academic community.",
    website: "https://deborah-diaz.com",
    twitter: "deborahdiaz",
    publications: [],
    recentPosts: [],
    collaborators: [],
  },
  {
    id: 18,
    slug: "rose-roberts",
    name: "Dr. Rose Roberts",
    avatar: "/portraits/imgs11.jpg",
    affiliation: "EleutherAI",
    communities: ["academic", "ea"],
    topics: ["AI Risk", "AI Alignment"],
    papers: 84,
    posts: 39,
    hIndex: 7,
    bio: "Researcher at Allen Institute for AI focusing on ai risk and ai alignment.",
    fullBio: "Dr. Rose Roberts is a researcher specializing in ai risk, ai alignment. They have published extensively in top-tier venues and are actively involved in the academic community.",
    website: "https://rose-roberts.com",
    twitter: "roseroberts",
    publications: [],
    recentPosts: [],
    collaborators: [],
  },
  {
    id: 19,
    slug: "bobby-brooks",
    name: "Dr. Bobby Brooks",
    avatar: "/portraits/imgs18.jpg",
    affiliation: "Anthropic",
    communities: ["lesswrong", "academic", "ea"],
    topics: ["Deep Learning", "AI Alignment"],
    papers: 7,
    posts: 48,
    hIndex: 41,
    bio: "Researcher at Stanford HAI focusing on deep learning and ai alignment.",
    fullBio: "Dr. Bobby Brooks is a researcher specializing in deep learning, ai alignment. They have published extensively in top-tier venues and are actively involved in the lesswrong community.",
    website: "https://bobby-brooks.com",
    twitter: "bobbybrooks",
    publications: [],
    recentPosts: [],
    collaborators: [],
  },
  {
    id: 20,
    slug: "wayne-jones",
    name: "Dr. Wayne Jones",
    avatar: "/portraits/imgs37.jpg",
    affiliation: "Allen Institute for AI",
    communities: ["lesswrong", "academic", "ea"],
    topics: ["Mechanistic Interpretability", "AI Risk", "RLHF"],
    papers: 57,
    posts: 28,
    hIndex: 24,
    bio: "Researcher at UC Berkeley focusing on mechanistic interpretability and ai risk.",
    fullBio: "Dr. Wayne Jones is a researcher specializing in mechanistic interpretability, ai risk, rlhf. They have published extensively in top-tier venues and are actively involved in the lesswrong community.",
    website: "https://wayne-jones.com",
    twitter: "waynejones",
    publications: [],
    recentPosts: [],
    collaborators: [],
  },
  {
    id: 21,
    slug: "logan-roberts",
    name: "Dr. Logan Roberts",
    avatar: "/portraits/imgs29.jpg",
    affiliation: "Stanford HAI",
    communities: ["lesswrong"],
    topics: ["Interpretability", "Deep Learning", "AI Risk"],
    papers: 41,
    posts: 41,
    hIndex: 22,
    bio: "Researcher at Oxford FHI focusing on interpretability and deep learning.",
    fullBio: "Dr. Logan Roberts is a researcher specializing in interpretability, deep learning, ai risk. They have published extensively in top-tier venues and are actively involved in the lesswrong community.",
    website: "https://logan-roberts.com",
    twitter: "loganroberts",
    publications: [],
    recentPosts: [],
    collaborators: [],
  },
  {
    id: 22,
    slug: "kathryn-garcia",
    name: "Dr. Kathryn Garcia",
    avatar: "/portraits/imgs44.jpg",
    affiliation: "Stanford HAI",
    communities: ["lesswrong"],
    topics: ["AI Alignment", "Deep Learning"],
    papers: 135,
    posts: 29,
    hIndex: 8,
    bio: "Researcher at Stanford HAI focusing on ai alignment and deep learning.",
    fullBio: "Dr. Kathryn Garcia is a researcher specializing in ai alignment, deep learning. They have published extensively in top-tier venues and are actively involved in the lesswrong community.",
    website: "https://kathryn-garcia.com",
    twitter: "kathryngarcia",
    publications: [],
    recentPosts: [],
    collaborators: [],
  },
  {
    id: 23,
    slug: "tyler-stewart",
    name: "Dr. Tyler Stewart",
    avatar: "/portraits/imgs21.jpg",
    affiliation: "Carnegie Mellon University",
    communities: ["lesswrong"],
    topics: ["AI Governance", "Value Alignment", "AI Risk", "Decision Theory"],
    papers: 33,
    posts: 40,
    hIndex: 42,
    bio: "Researcher at Allen Institute for AI focusing on ai governance and value alignment.",
    fullBio: "Dr. Tyler Stewart is a researcher specializing in ai governance, value alignment, ai risk, decision theory. They have published extensively in top-tier venues and are actively involved in the lesswrong community.",
    website: "https://tyler-stewart.com",
    twitter: "tylerstewart",
    publications: [],
    recentPosts: [],
    collaborators: [],
  },
  {
    id: 24,
    slug: "jack-martinez",
    name: "Dr. Jack Martinez",
    avatar: "/portraits/imgs27.jpg",
    affiliation: "Oxford FHI",
    communities: ["ea"],
    topics: ["Mechanistic Interpretability", "AI Governance", "RLHF"],
    papers: 71,
    posts: 33,
    hIndex: 42,
    bio: "Researcher at Oxford FHI focusing on mechanistic interpretability and ai governance.",
    fullBio: "Dr. Jack Martinez is a researcher specializing in mechanistic interpretability, ai governance, rlhf. They have published extensively in top-tier venues and are actively involved in the ea community.",
    website: "https://jack-martinez.com",
    twitter: "jackmartinez",
    publications: [],
    recentPosts: [],
    collaborators: [],
  },
  {
    id: 25,
    slug: "lawrence-watson",
    name: "Dr. Lawrence Watson",
    avatar: "/portraits/imgs32.jpg",
    affiliation: "Cambridge",
    communities: ["lesswrong", "academic", "ea"],
    topics: ["Decision Theory", "Value Alignment", "Interpretability", "Deep Learning"],
    papers: 8,
    posts: 12,
    hIndex: 28,
    bio: "Researcher at Allen Institute for AI focusing on decision theory and value alignment.",
    fullBio: "Dr. Lawrence Watson is a researcher specializing in decision theory, value alignment, interpretability, deep learning. They have published extensively in top-tier venues and are actively involved in the lesswrong community.",
    website: "https://lawrence-watson.com",
    twitter: "lawrencewatson",
    publications: [],
    recentPosts: [],
    collaborators: [],
  },
  {
    id: 26,
    slug: "denise-bennett",
    name: "Dr. Denise Bennett",
    avatar: "/portraits/imgs46.jpg",
    affiliation: "Carnegie Mellon University",
    communities: ["academic", "ea", "lesswrong"],
    topics: ["Decision Theory", "Deep Learning", "Value Alignment"],
    papers: 17,
    posts: 47,
    hIndex: 41,
    bio: "Researcher at Allen Institute for AI focusing on decision theory and deep learning.",
    fullBio: "Dr. Denise Bennett is a researcher specializing in decision theory, deep learning, value alignment. They have published extensively in top-tier venues and are actively involved in the academic community.",
    website: "https://denise-bennett.com",
    twitter: "denisebennett",
    publications: [],
    recentPosts: [],
    collaborators: [],
  },
  {
    id: 27,
    slug: "scott-lee",
    name: "Dr. Scott Lee",
    avatar: "/portraits/imgs4.jpg",
    affiliation: "OpenAI",
    communities: ["academic", "ea", "lesswrong"],
    topics: ["Mechanistic Interpretability", "Value Alignment", "Deep Learning"],
    papers: 14,
    posts: 39,
    hIndex: 41,
    bio: "Researcher at Carnegie Mellon University focusing on mechanistic interpretability and value alignment.",
    fullBio: "Dr. Scott Lee is a researcher specializing in mechanistic interpretability, value alignment, deep learning. They have published extensively in top-tier venues and are actively involved in the academic community.",
    website: "https://scott-lee.com",
    twitter: "scottlee",
    publications: [],
    recentPosts: [],
    collaborators: [],
  },
  {
    id: 28,
    slug: "russell-wood",
    name: "Dr. Russell Wood",
    avatar: "/portraits/imgs10.jpg",
    affiliation: "Google DeepMind",
    communities: ["academic"],
    topics: ["AI Risk", "RLHF", "Decision Theory"],
    papers: 142,
    posts: 42,
    hIndex: 18,
    bio: "Researcher at OpenAI focusing on ai risk and rlhf.",
    fullBio: "Dr. Russell Wood is a researcher specializing in ai risk, rlhf, decision theory. They have published extensively in top-tier venues and are actively involved in the academic community.",
    website: "https://russell-wood.com",
    twitter: "russellwood",
    publications: [],
    recentPosts: [],
    collaborators: [],
  },
  {
    id: 29,
    slug: "louis-henderson",
    name: "Dr. Louis Henderson",
    avatar: "/portraits/imgs29.jpg",
    affiliation: "Cambridge",
    communities: ["lesswrong"],
    topics: ["Value Alignment", "AI Risk"],
    papers: 65,
    posts: 28,
    hIndex: 38,
    bio: "Researcher at Cambridge focusing on value alignment and ai risk.",
    fullBio: "Dr. Louis Henderson is a researcher specializing in value alignment, ai risk. They have published extensively in top-tier venues and are actively involved in the lesswrong community.",
    website: "https://louis-henderson.com",
    twitter: "louishenderson",
    publications: [],
    recentPosts: [],
    collaborators: [],
  },
  {
    id: 30,
    slug: "arthur-coleman",
    name: "Dr. Arthur Coleman",
    avatar: "/portraits/imgs33.jpg",
    affiliation: "Carnegie Mellon University",
    communities: ["ea", "academic", "lesswrong"],
    topics: ["Interpretability", "Value Alignment", "AI Alignment"],
    papers: 126,
    posts: 40,
    hIndex: 37,
    bio: "Researcher at Cambridge focusing on interpretability and value alignment.",
    fullBio: "Dr. Arthur Coleman is a researcher specializing in interpretability, value alignment, ai alignment. They have published extensively in top-tier venues and are actively involved in the ea community.",
    website: "https://arthur-coleman.com",
    twitter: "arthurcoleman",
    publications: [],
    recentPosts: [],
    collaborators: [],
  },
  {
    id: 31,
    slug: "anna-taylor",
    name: "Dr. Anna Taylor",
    avatar: "/portraits/imgs38.jpg",
    affiliation: "Oxford FHI",
    communities: ["academic", "lesswrong"],
    topics: ["AI Risk", "RLHF", "Deep Learning", "Decision Theory"],
    papers: 90,
    posts: 30,
    hIndex: 56,
    bio: "Researcher at Anthropic focusing on ai risk and rlhf.",
    fullBio: "Dr. Anna Taylor is a researcher specializing in ai risk, rlhf, deep learning, decision theory. They have published extensively in top-tier venues and are actively involved in the academic community.",
    website: "https://anna-taylor.com",
    twitter: "annataylor",
    publications: [],
    recentPosts: [],
    collaborators: [],
  },
  {
    id: 32,
    slug: "paul-lopez",
    name: "Dr. Paul Lopez",
    avatar: "/portraits/imgs17.jpg",
    affiliation: "Cambridge",
    communities: ["ea"],
    topics: ["AI Governance", "AI Risk", "Decision Theory"],
    papers: 42,
    posts: 12,
    hIndex: 47,
    bio: "Researcher at Stanford HAI focusing on ai governance and ai risk.",
    fullBio: "Dr. Paul Lopez is a researcher specializing in ai governance, ai risk, decision theory. They have published extensively in top-tier venues and are actively involved in the ea community.",
    website: "https://paul-lopez.com",
    twitter: "paullopez",
    publications: [],
    recentPosts: [],
    collaborators: [],
  },
  {
    id: 33,
    slug: "marie-cox",
    name: "Dr. Marie Cox",
    avatar: "/portraits/imgs42.jpg",
    affiliation: "OpenAI",
    communities: ["ea"],
    topics: ["Deep Learning", "AI Risk", "RLHF", "Mechanistic Interpretability"],
    papers: 111,
    posts: 27,
    hIndex: 30,
    bio: "Researcher at Cambridge focusing on deep learning and ai risk.",
    fullBio: "Dr. Marie Cox is a researcher specializing in deep learning, ai risk, rlhf, mechanistic interpretability. They have published extensively in top-tier venues and are actively involved in the ea community.",
    website: "https://marie-cox.com",
    twitter: "mariecox",
    publications: [],
    recentPosts: [],
    collaborators: [],
  },
  {
    id: 34,
    slug: "nathan-moore",
    name: "Dr. Nathan Moore",
    avatar: "/portraits/imgs28.jpg",
    affiliation: "Carnegie Mellon University",
    communities: ["ea", "academic"],
    topics: ["Interpretability", "AI Governance", "Deep Learning"],
    papers: 105,
    posts: 26,
    hIndex: 23,
    bio: "Researcher at Stanford HAI focusing on interpretability and ai governance.",
    fullBio: "Dr. Nathan Moore is a researcher specializing in interpretability, ai governance, deep learning. They have published extensively in top-tier venues and are actively involved in the ea community.",
    website: "https://nathan-moore.com",
    twitter: "nathanmoore",
    publications: [],
    recentPosts: [],
    collaborators: [],
  },
  {
    id: 35,
    slug: "grace-bryant",
    name: "Dr. Grace Bryant",
    avatar: "/portraits/imgs30.jpg",
    affiliation: "Cambridge",
    communities: ["academic", "ea", "lesswrong"],
    topics: ["Value Alignment", "Deep Learning"],
    papers: 150,
    posts: 43,
    hIndex: 21,
    bio: "Researcher at MIRI focusing on value alignment and deep learning.",
    fullBio: "Dr. Grace Bryant is a researcher specializing in value alignment, deep learning. They have published extensively in top-tier venues and are actively involved in the academic community.",
    website: "https://grace-bryant.com",
    twitter: "gracebryant",
    publications: [],
    recentPosts: [],
    collaborators: [],
  },
  {
    id: 36,
    slug: "catherine-parker",
    name: "Dr. Catherine Parker",
    avatar: "/portraits/imgs31.jpg",
    affiliation: "Anthropic",
    communities: ["lesswrong"],
    topics: ["RLHF", "AI Governance", "Interpretability", "Value Alignment"],
    papers: 58,
    posts: 44,
    hIndex: 20,
    bio: "Researcher at University of Toronto focusing on rlhf and ai governance.",
    fullBio: "Dr. Catherine Parker is a researcher specializing in rlhf, ai governance, interpretability, value alignment. They have published extensively in top-tier venues and are actively involved in the lesswrong community.",
    website: "https://catherine-parker.com",
    twitter: "catherineparker",
    publications: [],
    recentPosts: [],
    collaborators: [],
  },
  {
    id: 37,
    slug: "emily-scott",
    name: "Dr. Emily Scott",
    avatar: "/portraits/imgs45.jpg",
    affiliation: "Allen Institute for AI",
    communities: ["lesswrong", "ea", "academic"],
    topics: ["Mechanistic Interpretability", "AI Alignment"],
    papers: 119,
    posts: 4,
    hIndex: 45,
    bio: "Researcher at Anthropic focusing on mechanistic interpretability and ai alignment.",
    fullBio: "Dr. Emily Scott is a researcher specializing in mechanistic interpretability, ai alignment. They have published extensively in top-tier venues and are actively involved in the lesswrong community.",
    website: "https://emily-scott.com",
    twitter: "emilyscott",
    publications: [],
    recentPosts: [],
    collaborators: [],
  },
  {
    id: 38,
    slug: "sandra-baker",
    name: "Dr. Sandra Baker",
    avatar: "/portraits/imgs12.jpg",
    affiliation: "Cambridge",
    communities: ["academic", "lesswrong", "ea"],
    topics: ["Interpretability", "Mechanistic Interpretability", "AI Governance"],
    papers: 108,
    posts: 36,
    hIndex: 49,
    bio: "Researcher at Anthropic focusing on interpretability and mechanistic interpretability.",
    fullBio: "Dr. Sandra Baker is a researcher specializing in interpretability, mechanistic interpretability, ai governance. They have published extensively in top-tier venues and are actively involved in the academic community.",
    website: "https://sandra-baker.com",
    twitter: "sandrabaker",
    publications: [],
    recentPosts: [],
    collaborators: [],
  },
  {
    id: 39,
    slug: "amanda-harris",
    name: "Dr. Amanda Harris",
    avatar: "/portraits/imgs32.jpg",
    affiliation: "Oxford FHI",
    communities: ["academic", "ea"],
    topics: ["AI Alignment", "AI Governance", "Deep Learning"],
    papers: 16,
    posts: 15,
    hIndex: 58,
    bio: "Researcher at EleutherAI focusing on ai alignment and ai governance.",
    fullBio: "Dr. Amanda Harris is a researcher specializing in ai alignment, ai governance, deep learning. They have published extensively in top-tier venues and are actively involved in the academic community.",
    website: "https://amanda-harris.com",
    twitter: "amandaharris",
    publications: [],
    recentPosts: [],
    collaborators: [],
  },
  {
    id: 40,
    slug: "alan-gonzalez",
    name: "Dr. Alan Gonzalez",
    avatar: "/portraits/imgs43.jpg",
    affiliation: "OpenAI",
    communities: ["academic", "lesswrong"],
    topics: ["Interpretability", "AI Risk", "AI Governance"],
    papers: 132,
    posts: 34,
    hIndex: 30,
    bio: "Researcher at Google DeepMind focusing on interpretability and ai risk.",
    fullBio: "Dr. Alan Gonzalez is a researcher specializing in interpretability, ai risk, ai governance. They have published extensively in top-tier venues and are actively involved in the academic community.",
    website: "https://alan-gonzalez.com",
    twitter: "alangonzalez",
    publications: [],
    recentPosts: [],
    collaborators: [],
  },
  {
    id: 41,
    slug: "charlotte-morris",
    name: "Dr. Charlotte Morris",
    avatar: "/portraits/imgs32.jpg",
    affiliation: "Stanford HAI",
    communities: ["ea", "academic"],
    topics: ["AI Alignment", "Value Alignment", "Mechanistic Interpretability"],
    papers: 53,
    posts: 34,
    hIndex: 8,
    bio: "Researcher at MIRI focusing on ai alignment and value alignment.",
    fullBio: "Dr. Charlotte Morris is a researcher specializing in ai alignment, value alignment, mechanistic interpretability. They have published extensively in top-tier venues and are actively involved in the ea community.",
    website: "https://charlotte-morris.com",
    twitter: "charlottemorris",
    publications: [],
    recentPosts: [],
    collaborators: [],
  },
  {
    id: 42,
    slug: "sharon-ramirez",
    name: "Dr. Sharon Ramirez",
    avatar: "/portraits/imgs29.jpg",
    affiliation: "Anthropic",
    communities: ["academic"],
    topics: ["Deep Learning", "AI Risk", "AI Governance"],
    papers: 74,
    posts: 21,
    hIndex: 40,
    bio: "Researcher at UC Berkeley focusing on deep learning and ai risk.",
    fullBio: "Dr. Sharon Ramirez is a researcher specializing in deep learning, ai risk, ai governance. They have published extensively in top-tier venues and are actively involved in the academic community.",
    website: "https://sharon-ramirez.com",
    twitter: "sharonramirez",
    publications: [],
    recentPosts: [],
    collaborators: [],
  },
  {
    id: 43,
    slug: "philip-baker",
    name: "Dr. Philip Baker",
    avatar: "/portraits/imgs9.jpg",
    affiliation: "Anthropic",
    communities: ["lesswrong", "ea"],
    topics: ["AI Risk", "Value Alignment"],
    papers: 64,
    posts: 42,
    hIndex: 36,
    bio: "Researcher at MIT CSAIL focusing on ai risk and value alignment.",
    fullBio: "Dr. Philip Baker is a researcher specializing in ai risk, value alignment. They have published extensively in top-tier venues and are actively involved in the lesswrong community.",
    website: "https://philip-baker.com",
    twitter: "philipbaker",
    publications: [],
    recentPosts: [],
    collaborators: [],
  },
  {
    id: 44,
    slug: "benjamin-kelly",
    name: "Dr. Benjamin Kelly",
    avatar: "/portraits/imgs32.jpg",
    affiliation: "Carnegie Mellon University",
    communities: ["ea", "lesswrong"],
    topics: ["Mechanistic Interpretability", "Decision Theory", "AI Alignment", "Value Alignment"],
    papers: 132,
    posts: 44,
    hIndex: 9,
    bio: "Researcher at Oxford FHI focusing on mechanistic interpretability and decision theory.",
    fullBio: "Dr. Benjamin Kelly is a researcher specializing in mechanistic interpretability, decision theory, ai alignment, value alignment. They have published extensively in top-tier venues and are actively involved in the ea community.",
    website: "https://benjamin-kelly.com",
    twitter: "benjaminkelly",
    publications: [],
    recentPosts: [],
    collaborators: [],
  },
  {
    id: 45,
    slug: "barbara-cook",
    name: "Dr. Barbara Cook",
    avatar: "/portraits/imgs30.jpg",
    affiliation: "University of Toronto",
    communities: ["lesswrong", "ea", "academic"],
    topics: ["Mechanistic Interpretability", "RLHF"],
    papers: 138,
    posts: 48,
    hIndex: 6,
    bio: "Researcher at Google DeepMind focusing on mechanistic interpretability and rlhf.",
    fullBio: "Dr. Barbara Cook is a researcher specializing in mechanistic interpretability, rlhf. They have published extensively in top-tier venues and are actively involved in the lesswrong community.",
    website: "https://barbara-cook.com",
    twitter: "barbaracook",
    publications: [],
    recentPosts: [],
    collaborators: [],
  },
  {
    id: 46,
    slug: "mary-alexander",
    name: "Dr. Mary Alexander",
    avatar: "/portraits/imgs32.jpg",
    affiliation: "EleutherAI",
    communities: ["academic"],
    topics: ["AI Risk", "Deep Learning", "Decision Theory", "AI Governance"],
    papers: 14,
    posts: 36,
    hIndex: 26,
    bio: "Researcher at University of Toronto focusing on ai risk and deep learning.",
    fullBio: "Dr. Mary Alexander is a researcher specializing in ai risk, deep learning, decision theory, ai governance. They have published extensively in top-tier venues and are actively involved in the academic community.",
    website: "https://mary-alexander.com",
    twitter: "maryalexander",
    publications: [],
    recentPosts: [],
    collaborators: [],
  },
  {
    id: 47,
    slug: "lisa-rodriguez",
    name: "Dr. Lisa Rodriguez",
    avatar: "/portraits/imgs42.jpg",
    affiliation: "EleutherAI",
    communities: ["ea"],
    topics: ["AI Governance", "AI Alignment"],
    papers: 63,
    posts: 12,
    hIndex: 44,
    bio: "Researcher at Cambridge focusing on ai governance and ai alignment.",
    fullBio: "Dr. Lisa Rodriguez is a researcher specializing in ai governance, ai alignment. They have published extensively in top-tier venues and are actively involved in the ea community.",
    website: "https://lisa-rodriguez.com",
    twitter: "lisarodriguez",
    publications: [],
    recentPosts: [],
    collaborators: [],
  },
  {
    id: 48,
    slug: "bobby-perez",
    name: "Dr. Bobby Perez",
    avatar: "/portraits/imgs16.jpg",
    affiliation: "University of Toronto",
    communities: ["ea", "lesswrong", "academic"],
    topics: ["Decision Theory", "AI Alignment", "Interpretability"],
    papers: 64,
    posts: 34,
    hIndex: 20,
    bio: "Researcher at Stanford HAI focusing on decision theory and ai alignment.",
    fullBio: "Dr. Bobby Perez is a researcher specializing in decision theory, ai alignment, interpretability. They have published extensively in top-tier venues and are actively involved in the ea community.",
    website: "https://bobby-perez.com",
    twitter: "bobbyperez",
    publications: [],
    recentPosts: [],
    collaborators: [],
  },
  {
    id: 49,
    slug: "rebecca-price",
    name: "Dr. Rebecca Price",
    avatar: "/portraits/imgs22.jpg",
    affiliation: "MIRI",
    communities: ["academic", "lesswrong", "ea"],
    topics: ["Interpretability", "Mechanistic Interpretability"],
    papers: 139,
    posts: 12,
    hIndex: 31,
    bio: "Researcher at Stanford HAI focusing on interpretability and mechanistic interpretability.",
    fullBio: "Dr. Rebecca Price is a researcher specializing in interpretability, mechanistic interpretability. They have published extensively in top-tier venues and are actively involved in the academic community.",
    website: "https://rebecca-price.com",
    twitter: "rebeccaprice",
    publications: [],
    recentPosts: [],
    collaborators: [],
  },
  {
    id: 50,
    slug: "terry-butler",
    name: "Dr. Terry Butler",
    avatar: "/portraits/imgs45.jpg",
    affiliation: "OpenAI",
    communities: ["lesswrong"],
    topics: ["Deep Learning", "AI Governance", "AI Risk", "AI Alignment"],
    papers: 141,
    posts: 45,
    hIndex: 31,
    bio: "Researcher at Oxford FHI focusing on deep learning and ai governance.",
    fullBio: "Dr. Terry Butler is a researcher specializing in deep learning, ai governance, ai risk, ai alignment. They have published extensively in top-tier venues and are actively involved in the lesswrong community.",
    website: "https://terry-butler.com",
    twitter: "terrybutler",
    publications: [],
    recentPosts: [],
    collaborators: [],
  },
  {
    id: 51,
    slug: "jonathan-bailey",
    name: "Dr. Jonathan Bailey",
    avatar: "/portraits/imgs7.jpg",
    affiliation: "MIRI",
    communities: ["lesswrong", "ea"],
    topics: ["Mechanistic Interpretability", "Decision Theory", "Deep Learning", "Interpretability"],
    papers: 46,
    posts: 12,
    hIndex: 44,
    bio: "Researcher at Oxford FHI focusing on mechanistic interpretability and decision theory.",
    fullBio: "Dr. Jonathan Bailey is a researcher specializing in mechanistic interpretability, decision theory, deep learning, interpretability. They have published extensively in top-tier venues and are actively involved in the lesswrong community.",
    website: "https://jonathan-bailey.com",
    twitter: "jonathanbailey",
    publications: [],
    recentPosts: [],
    collaborators: [],
  },
  {
    id: 52,
    slug: "margaret-morgan",
    name: "Dr. Margaret Morgan",
    avatar: "/portraits/imgs15.jpg",
    affiliation: "Carnegie Mellon University",
    communities: ["ea"],
    topics: ["Mechanistic Interpretability", "AI Governance", "Interpretability", "Deep Learning"],
    papers: 101,
    posts: 18,
    hIndex: 20,
    bio: "Researcher at Oxford FHI focusing on mechanistic interpretability and ai governance.",
    fullBio: "Dr. Margaret Morgan is a researcher specializing in mechanistic interpretability, ai governance, interpretability, deep learning. They have published extensively in top-tier venues and are actively involved in the ea community.",
    website: "https://margaret-morgan.com",
    twitter: "margaretmorgan",
    publications: [],
    recentPosts: [],
    collaborators: [],
  },
  {
    id: 53,
    slug: "christian-brown",
    name: "Dr. Christian Brown",
    avatar: "/portraits/imgs20.jpg",
    affiliation: "Cambridge",
    communities: ["academic"],
    topics: ["Value Alignment", "AI Risk"],
    papers: 90,
    posts: 14,
    hIndex: 59,
    bio: "Researcher at Oxford FHI focusing on value alignment and ai risk.",
    fullBio: "Dr. Christian Brown is a researcher specializing in value alignment, ai risk. They have published extensively in top-tier venues and are actively involved in the academic community.",
    website: "https://christian-brown.com",
    twitter: "christianbrown",
    publications: [],
    recentPosts: [],
    collaborators: [],
  },
  {
    id: 54,
    slug: "david-wilson",
    name: "Dr. David Wilson",
    avatar: "/portraits/imgs18.jpg",
    affiliation: "MIRI",
    communities: ["lesswrong"],
    topics: ["AI Risk", "Deep Learning"],
    papers: 44,
    posts: 24,
    hIndex: 34,
    bio: "Researcher at Allen Institute for AI focusing on ai risk and deep learning.",
    fullBio: "Dr. David Wilson is a researcher specializing in ai risk, deep learning. They have published extensively in top-tier venues and are actively involved in the lesswrong community.",
    website: "https://david-wilson.com",
    twitter: "davidwilson",
    publications: [],
    recentPosts: [],
    collaborators: [],
  },
  {
    id: 55,
    slug: "amanda-russell",
    name: "Dr. Amanda Russell",
    avatar: "/portraits/imgs9.jpg",
    affiliation: "Stanford HAI",
    communities: ["academic"],
    topics: ["RLHF", "Value Alignment", "Deep Learning"],
    papers: 73,
    posts: 31,
    hIndex: 22,
    bio: "Researcher at Anthropic focusing on rlhf and value alignment.",
    fullBio: "Dr. Amanda Russell is a researcher specializing in rlhf, value alignment, deep learning. They have published extensively in top-tier venues and are actively involved in the academic community.",
    website: "https://amanda-russell.com",
    twitter: "amandarussell",
    publications: [],
    recentPosts: [],
    collaborators: [],
  },
  {
    id: 56,
    slug: "jennifer-coleman",
    name: "Dr. Jennifer Coleman",
    avatar: "/portraits/imgs32.jpg",
    affiliation: "Oxford FHI",
    communities: ["academic"],
    topics: ["Mechanistic Interpretability", "AI Governance"],
    papers: 109,
    posts: 40,
    hIndex: 60,
    bio: "Researcher at Anthropic focusing on mechanistic interpretability and ai governance.",
    fullBio: "Dr. Jennifer Coleman is a researcher specializing in mechanistic interpretability, ai governance. They have published extensively in top-tier venues and are actively involved in the academic community.",
    website: "https://jennifer-coleman.com",
    twitter: "jennifercoleman",
    publications: [],
    recentPosts: [],
    collaborators: [],
  },
  {
    id: 57,
    slug: "marilyn-price",
    name: "Dr. Marilyn Price",
    avatar: "/portraits/imgs20.jpg",
    affiliation: "Allen Institute for AI",
    communities: ["lesswrong"],
    topics: ["Interpretability", "Mechanistic Interpretability", "AI Risk", "Deep Learning"],
    papers: 112,
    posts: 12,
    hIndex: 10,
    bio: "Researcher at MIRI focusing on interpretability and mechanistic interpretability.",
    fullBio: "Dr. Marilyn Price is a researcher specializing in interpretability, mechanistic interpretability, ai risk, deep learning. They have published extensively in top-tier venues and are actively involved in the lesswrong community.",
    website: "https://marilyn-price.com",
    twitter: "marilynprice",
    publications: [],
    recentPosts: [],
    collaborators: [],
  },
  {
    id: 58,
    slug: "joan-martinez",
    name: "Dr. Joan Martinez",
    avatar: "/portraits/imgs46.jpg",
    affiliation: "Carnegie Mellon University",
    communities: ["academic", "ea"],
    topics: ["AI Alignment", "AI Governance", "Decision Theory"],
    papers: 39,
    posts: 28,
    hIndex: 29,
    bio: "Researcher at Allen Institute for AI focusing on ai alignment and ai governance.",
    fullBio: "Dr. Joan Martinez is a researcher specializing in ai alignment, ai governance, decision theory. They have published extensively in top-tier venues and are actively involved in the academic community.",
    website: "https://joan-martinez.com",
    twitter: "joanmartinez",
    publications: [],
    recentPosts: [],
    collaborators: [],
  },
  {
    id: 59,
    slug: "beverly-bailey",
    name: "Dr. Beverly Bailey",
    avatar: "/portraits/imgs22.jpg",
    affiliation: "University of Toronto",
    communities: ["ea", "lesswrong"],
    topics: ["Value Alignment", "Mechanistic Interpretability"],
    papers: 60,
    posts: 21,
    hIndex: 5,
    bio: "Researcher at MIT CSAIL focusing on value alignment and mechanistic interpretability.",
    fullBio: "Dr. Beverly Bailey is a researcher specializing in value alignment, mechanistic interpretability. They have published extensively in top-tier venues and are actively involved in the ea community.",
    website: "https://beverly-bailey.com",
    twitter: "beverlybailey",
    publications: [],
    recentPosts: [],
    collaborators: [],
  },
  {
    id: 60,
    slug: "jack-patterson",
    name: "Dr. Jack Patterson",
    avatar: "/portraits/imgs35.jpg",
    affiliation: "MIT CSAIL",
    communities: ["lesswrong", "ea", "academic"],
    topics: ["Deep Learning", "AI Risk", "Value Alignment", "AI Governance"],
    papers: 103,
    posts: 14,
    hIndex: 53,
    bio: "Researcher at OpenAI focusing on deep learning and ai risk.",
    fullBio: "Dr. Jack Patterson is a researcher specializing in deep learning, ai risk, value alignment, ai governance. They have published extensively in top-tier venues and are actively involved in the lesswrong community.",
    website: "https://jack-patterson.com",
    twitter: "jackpatterson",
    publications: [],
    recentPosts: [],
    collaborators: [],
  },
  {
    id: 61,
    slug: "sean-patterson",
    name: "Dr. Sean Patterson",
    avatar: "/portraits/imgs26.jpg",
    affiliation: "OpenAI",
    communities: ["academic", "ea", "lesswrong"],
    topics: ["AI Risk", "AI Governance"],
    papers: 68,
    posts: 18,
    hIndex: 26,
    bio: "Researcher at Cambridge focusing on ai risk and ai governance.",
    fullBio: "Dr. Sean Patterson is a researcher specializing in ai risk, ai governance. They have published extensively in top-tier venues and are actively involved in the academic community.",
    website: "https://sean-patterson.com",
    twitter: "seanpatterson",
    publications: [],
    recentPosts: [],
    collaborators: [],
  },
  {
    id: 62,
    slug: "logan-adams",
    name: "Dr. Logan Adams",
    avatar: "/portraits/imgs45.jpg",
    affiliation: "MIRI",
    communities: ["ea", "lesswrong"],
    topics: ["AI Alignment", "Interpretability", "AI Governance"],
    papers: 21,
    posts: 13,
    hIndex: 54,
    bio: "Researcher at EleutherAI focusing on ai alignment and interpretability.",
    fullBio: "Dr. Logan Adams is a researcher specializing in ai alignment, interpretability, ai governance. They have published extensively in top-tier venues and are actively involved in the ea community.",
    website: "https://logan-adams.com",
    twitter: "loganadams",
    publications: [],
    recentPosts: [],
    collaborators: [],
  },
  {
    id: 63,
    slug: "ann-hayes",
    name: "Dr. Ann Hayes",
    avatar: "/portraits/imgs28.jpg",
    affiliation: "EleutherAI",
    communities: ["ea", "lesswrong", "academic"],
    topics: ["Decision Theory", "Value Alignment"],
    papers: 93,
    posts: 3,
    hIndex: 36,
    bio: "Researcher at Stanford HAI focusing on decision theory and value alignment.",
    fullBio: "Dr. Ann Hayes is a researcher specializing in decision theory, value alignment. They have published extensively in top-tier venues and are actively involved in the ea community.",
    website: "https://ann-hayes.com",
    twitter: "annhayes",
    publications: [],
    recentPosts: [],
    collaborators: [],
  },
  {
    id: 64,
    slug: "kenneth-hernandez",
    name: "Dr. Kenneth Hernandez",
    avatar: "/portraits/imgs47.jpg",
    affiliation: "Anthropic",
    communities: ["academic"],
    topics: ["AI Risk", "Interpretability", "Mechanistic Interpretability"],
    papers: 71,
    posts: 4,
    hIndex: 42,
    bio: "Researcher at Allen Institute for AI focusing on ai risk and interpretability.",
    fullBio: "Dr. Kenneth Hernandez is a researcher specializing in ai risk, interpretability, mechanistic interpretability. They have published extensively in top-tier venues and are actively involved in the academic community.",
    website: "https://kenneth-hernandez.com",
    twitter: "kennethhernandez",
    publications: [],
    recentPosts: [],
    collaborators: [],
  },
  {
    id: 65,
    slug: "jordan-morgan",
    name: "Dr. Jordan Morgan",
    avatar: "/portraits/imgs48.jpg",
    affiliation: "Allen Institute for AI",
    communities: ["lesswrong", "ea"],
    topics: ["AI Risk", "Interpretability"],
    papers: 61,
    posts: 1,
    hIndex: 23,
    bio: "Researcher at OpenAI focusing on ai risk and interpretability.",
    fullBio: "Dr. Jordan Morgan is a researcher specializing in ai risk, interpretability. They have published extensively in top-tier venues and are actively involved in the lesswrong community.",
    website: "https://jordan-morgan.com",
    twitter: "jordanmorgan",
    publications: [],
    recentPosts: [],
    collaborators: [],
  },
  {
    id: 66,
    slug: "angela-rivera",
    name: "Dr. Angela Rivera",
    avatar: "/portraits/imgs44.jpg",
    affiliation: "EleutherAI",
    communities: ["academic"],
    topics: ["Decision Theory", "Mechanistic Interpretability", "AI Governance", "Deep Learning"],
    papers: 144,
    posts: 5,
    hIndex: 41,
    bio: "Researcher at Anthropic focusing on decision theory and mechanistic interpretability.",
    fullBio: "Dr. Angela Rivera is a researcher specializing in decision theory, mechanistic interpretability, ai governance, deep learning. They have published extensively in top-tier venues and are actively involved in the academic community.",
    website: "https://angela-rivera.com",
    twitter: "angelarivera",
    publications: [],
    recentPosts: [],
    collaborators: [],
  },
  {
    id: 67,
    slug: "john-hill",
    name: "Dr. John Hill",
    avatar: "/portraits/imgs2.jpg",
    affiliation: "MIRI",
    communities: ["lesswrong"],
    topics: ["Interpretability", "Mechanistic Interpretability", "Decision Theory", "Deep Learning"],
    papers: 27,
    posts: 17,
    hIndex: 40,
    bio: "Researcher at Google DeepMind focusing on interpretability and mechanistic interpretability.",
    fullBio: "Dr. John Hill is a researcher specializing in interpretability, mechanistic interpretability, decision theory, deep learning. They have published extensively in top-tier venues and are actively involved in the lesswrong community.",
    website: "https://john-hill.com",
    twitter: "johnhill",
    publications: [],
    recentPosts: [],
    collaborators: [],
  },
  {
    id: 68,
    slug: "arthur-perry",
    name: "Dr. Arthur Perry",
    avatar: "/portraits/imgs36.jpg",
    affiliation: "Allen Institute for AI",
    communities: ["ea", "lesswrong", "academic"],
    topics: ["Mechanistic Interpretability", "Interpretability"],
    papers: 79,
    posts: 26,
    hIndex: 8,
    bio: "Researcher at OpenAI focusing on mechanistic interpretability and interpretability.",
    fullBio: "Dr. Arthur Perry is a researcher specializing in mechanistic interpretability, interpretability. They have published extensively in top-tier venues and are actively involved in the ea community.",
    website: "https://arthur-perry.com",
    twitter: "arthurperry",
    publications: [],
    recentPosts: [],
    collaborators: [],
  },
  {
    id: 69,
    slug: "lawrence-martinez",
    name: "Dr. Lawrence Martinez",
    avatar: "/portraits/imgs41.jpg",
    affiliation: "EleutherAI",
    communities: ["academic", "ea", "lesswrong"],
    topics: ["AI Alignment", "Interpretability", "Mechanistic Interpretability"],
    papers: 124,
    posts: 31,
    hIndex: 25,
    bio: "Researcher at Allen Institute for AI focusing on ai alignment and interpretability.",
    fullBio: "Dr. Lawrence Martinez is a researcher specializing in ai alignment, interpretability, mechanistic interpretability. They have published extensively in top-tier venues and are actively involved in the academic community.",
    website: "https://lawrence-martinez.com",
    twitter: "lawrencemartinez",
    publications: [],
    recentPosts: [],
    collaborators: [],
  },
  {
    id: 70,
    slug: "ronald-wright",
    name: "Dr. Ronald Wright",
    avatar: "/portraits/imgs7.jpg",
    affiliation: "Anthropic",
    communities: ["academic", "ea"],
    topics: ["Decision Theory", "AI Governance", "AI Alignment"],
    papers: 97,
    posts: 31,
    hIndex: 15,
    bio: "Researcher at Allen Institute for AI focusing on decision theory and ai governance.",
    fullBio: "Dr. Ronald Wright is a researcher specializing in decision theory, ai governance, ai alignment. They have published extensively in top-tier venues and are actively involved in the academic community.",
    website: "https://ronald-wright.com",
    twitter: "ronaldwright",
    publications: [],
    recentPosts: [],
    collaborators: [],
  },
  {
    id: 71,
    slug: "samantha-scott",
    name: "Dr. Samantha Scott",
    avatar: "/portraits/imgs40.jpg",
    affiliation: "Anthropic",
    communities: ["ea", "lesswrong", "academic"],
    topics: ["Decision Theory", "Interpretability", "AI Governance"],
    papers: 11,
    posts: 27,
    hIndex: 38,
    bio: "Researcher at Carnegie Mellon University focusing on decision theory and interpretability.",
    fullBio: "Dr. Samantha Scott is a researcher specializing in decision theory, interpretability, ai governance. They have published extensively in top-tier venues and are actively involved in the ea community.",
    website: "https://samantha-scott.com",
    twitter: "samanthascott",
    publications: [],
    recentPosts: [],
    collaborators: [],
  },
  {
    id: 72,
    slug: "roy-bailey",
    name: "Dr. Roy Bailey",
    avatar: "/portraits/imgs40.jpg",
    affiliation: "Oxford FHI",
    communities: ["lesswrong", "ea", "academic"],
    topics: ["Decision Theory", "Deep Learning", "AI Governance", "Value Alignment"],
    papers: 74,
    posts: 6,
    hIndex: 14,
    bio: "Researcher at MIRI focusing on decision theory and deep learning.",
    fullBio: "Dr. Roy Bailey is a researcher specializing in decision theory, deep learning, ai governance, value alignment. They have published extensively in top-tier venues and are actively involved in the lesswrong community.",
    website: "https://roy-bailey.com",
    twitter: "roybailey",
    publications: [],
    recentPosts: [],
    collaborators: [],
  },
  {
    id: 73,
    slug: "teresa-howard",
    name: "Dr. Teresa Howard",
    avatar: "/portraits/imgs31.jpg",
    affiliation: "MIT CSAIL",
    communities: ["ea", "academic"],
    topics: ["AI Risk", "RLHF"],
    papers: 76,
    posts: 31,
    hIndex: 33,
    bio: "Researcher at OpenAI focusing on ai risk and rlhf.",
    fullBio: "Dr. Teresa Howard is a researcher specializing in ai risk, rlhf. They have published extensively in top-tier venues and are actively involved in the ea community.",
    website: "https://teresa-howard.com",
    twitter: "teresahoward",
    publications: [],
    recentPosts: [],
    collaborators: [],
  },
  {
    id: 74,
    slug: "jason-howard",
    name: "Dr. Jason Howard",
    avatar: "/portraits/imgs2.jpg",
    affiliation: "Oxford FHI",
    communities: ["lesswrong"],
    topics: ["AI Alignment", "Value Alignment"],
    papers: 82,
    posts: 24,
    hIndex: 17,
    bio: "Researcher at OpenAI focusing on ai alignment and value alignment.",
    fullBio: "Dr. Jason Howard is a researcher specializing in ai alignment, value alignment. They have published extensively in top-tier venues and are actively involved in the lesswrong community.",
    website: "https://jason-howard.com",
    twitter: "jasonhoward",
    publications: [],
    recentPosts: [],
    collaborators: [],
  },
  {
    id: 75,
    slug: "linda-kelly",
    name: "Dr. Linda Kelly",
    avatar: "/portraits/imgs38.jpg",
    affiliation: "MIT CSAIL",
    communities: ["lesswrong", "academic", "ea"],
    topics: ["Interpretability", "Deep Learning", "Value Alignment"],
    papers: 102,
    posts: 24,
    hIndex: 51,
    bio: "Researcher at UC Berkeley focusing on interpretability and deep learning.",
    fullBio: "Dr. Linda Kelly is a researcher specializing in interpretability, deep learning, value alignment. They have published extensively in top-tier venues and are actively involved in the lesswrong community.",
    website: "https://linda-kelly.com",
    twitter: "lindakelly",
    publications: [],
    recentPosts: [],
    collaborators: [],
  },
  {
    id: 76,
    slug: "elijah-anderson",
    name: "Dr. Elijah Anderson",
    avatar: "/portraits/imgs43.jpg",
    affiliation: "Oxford FHI",
    communities: ["lesswrong", "ea", "academic"],
    topics: ["Deep Learning", "AI Alignment"],
    papers: 66,
    posts: 2,
    hIndex: 26,
    bio: "Researcher at MIRI focusing on deep learning and ai alignment.",
    fullBio: "Dr. Elijah Anderson is a researcher specializing in deep learning, ai alignment. They have published extensively in top-tier venues and are actively involved in the lesswrong community.",
    website: "https://elijah-anderson.com",
    twitter: "elijahanderson",
    publications: [],
    recentPosts: [],
    collaborators: [],
  },
  {
    id: 77,
    slug: "richard-gonzalez",
    name: "Dr. Richard Gonzalez",
    avatar: "/portraits/imgs24.jpg",
    affiliation: "OpenAI",
    communities: ["academic", "ea", "lesswrong"],
    topics: ["AI Alignment", "Decision Theory", "AI Risk"],
    papers: 97,
    posts: 26,
    hIndex: 57,
    bio: "Researcher at EleutherAI focusing on ai alignment and decision theory.",
    fullBio: "Dr. Richard Gonzalez is a researcher specializing in ai alignment, decision theory, ai risk. They have published extensively in top-tier venues and are actively involved in the academic community.",
    website: "https://richard-gonzalez.com",
    twitter: "richardgonzalez",
    publications: [],
    recentPosts: [],
    collaborators: [],
  },
  {
    id: 78,
    slug: "nancy-reed",
    name: "Dr. Nancy Reed",
    avatar: "/portraits/imgs23.jpg",
    affiliation: "MIRI",
    communities: ["academic", "lesswrong", "ea"],
    topics: ["AI Risk", "Deep Learning", "Interpretability"],
    papers: 76,
    posts: 30,
    hIndex: 33,
    bio: "Researcher at University of Toronto focusing on ai risk and deep learning.",
    fullBio: "Dr. Nancy Reed is a researcher specializing in ai risk, deep learning, interpretability. They have published extensively in top-tier venues and are actively involved in the academic community.",
    website: "https://nancy-reed.com",
    twitter: "nancyreed",
    publications: [],
    recentPosts: [],
    collaborators: [],
  },
  {
    id: 79,
    slug: "matthew-bennett",
    name: "Dr. Matthew Bennett",
    avatar: "/portraits/imgs1.jpg",
    affiliation: "Anthropic",
    communities: ["academic", "ea"],
    topics: ["Decision Theory", "Mechanistic Interpretability", "Interpretability"],
    papers: 32,
    posts: 43,
    hIndex: 12,
    bio: "Researcher at MIRI focusing on decision theory and mechanistic interpretability.",
    fullBio: "Dr. Matthew Bennett is a researcher specializing in decision theory, mechanistic interpretability, interpretability. They have published extensively in top-tier venues and are actively involved in the academic community.",
    website: "https://matthew-bennett.com",
    twitter: "matthewbennett",
    publications: [],
    recentPosts: [],
    collaborators: [],
  },
  {
    id: 80,
    slug: "christian-wood",
    name: "Dr. Christian Wood",
    avatar: "/portraits/imgs2.jpg",
    affiliation: "University of Toronto",
    communities: ["ea", "academic"],
    topics: ["Mechanistic Interpretability", "AI Alignment"],
    papers: 136,
    posts: 16,
    hIndex: 50,
    bio: "Researcher at Allen Institute for AI focusing on mechanistic interpretability and ai alignment.",
    fullBio: "Dr. Christian Wood is a researcher specializing in mechanistic interpretability, ai alignment. They have published extensively in top-tier venues and are actively involved in the ea community.",
    website: "https://christian-wood.com",
    twitter: "christianwood",
    publications: [],
    recentPosts: [],
    collaborators: [],
  },
  {
    id: 81,
    slug: "donald-campbell",
    name: "Dr. Donald Campbell",
    avatar: "/portraits/imgs39.jpg",
    affiliation: "Cambridge",
    communities: ["academic", "lesswrong", "ea"],
    topics: ["Value Alignment", "AI Alignment", "AI Risk", "Interpretability"],
    papers: 138,
    posts: 5,
    hIndex: 50,
    bio: "Researcher at Allen Institute for AI focusing on value alignment and ai alignment.",
    fullBio: "Dr. Donald Campbell is a researcher specializing in value alignment, ai alignment, ai risk, interpretability. They have published extensively in top-tier venues and are actively involved in the academic community.",
    website: "https://donald-campbell.com",
    twitter: "donaldcampbell",
    publications: [],
    recentPosts: [],
    collaborators: [],
  },
  {
    id: 82,
    slug: "brittany-russell",
    name: "Dr. Brittany Russell",
    avatar: "/portraits/imgs17.jpg",
    affiliation: "MIRI",
    communities: ["ea", "lesswrong"],
    topics: ["Value Alignment", "AI Governance", "Decision Theory", "RLHF"],
    papers: 140,
    posts: 7,
    hIndex: 29,
    bio: "Researcher at EleutherAI focusing on value alignment and ai governance.",
    fullBio: "Dr. Brittany Russell is a researcher specializing in value alignment, ai governance, decision theory, rlhf. They have published extensively in top-tier venues and are actively involved in the ea community.",
    website: "https://brittany-russell.com",
    twitter: "brittanyrussell",
    publications: [],
    recentPosts: [],
    collaborators: [],
  },
  {
    id: 83,
    slug: "donna-baker",
    name: "Dr. Donna Baker",
    avatar: "/portraits/imgs13.jpg",
    affiliation: "OpenAI",
    communities: ["academic", "lesswrong"],
    topics: ["Deep Learning", "AI Risk", "Value Alignment", "Decision Theory"],
    papers: 77,
    posts: 41,
    hIndex: 20,
    bio: "Researcher at MIT CSAIL focusing on deep learning and ai risk.",
    fullBio: "Dr. Donna Baker is a researcher specializing in deep learning, ai risk, value alignment, decision theory. They have published extensively in top-tier venues and are actively involved in the academic community.",
    website: "https://donna-baker.com",
    twitter: "donnabaker",
    publications: [],
    recentPosts: [],
    collaborators: [],
  },
  {
    id: 84,
    slug: "ronald-brooks",
    name: "Dr. Ronald Brooks",
    avatar: "/portraits/imgs24.jpg",
    affiliation: "UC Berkeley",
    communities: ["academic"],
    topics: ["Interpretability", "AI Alignment", "Decision Theory", "Value Alignment"],
    papers: 12,
    posts: 9,
    hIndex: 43,
    bio: "Researcher at MIRI focusing on interpretability and ai alignment.",
    fullBio: "Dr. Ronald Brooks is a researcher specializing in interpretability, ai alignment, decision theory, value alignment. They have published extensively in top-tier venues and are actively involved in the academic community.",
    website: "https://ronald-brooks.com",
    twitter: "ronaldbrooks",
    publications: [],
    recentPosts: [],
    collaborators: [],
  },
  {
    id: 85,
    slug: "johnny-brown",
    name: "Dr. Johnny Brown",
    avatar: "/portraits/imgs23.jpg",
    affiliation: "Cambridge",
    communities: ["ea", "academic", "lesswrong"],
    topics: ["Value Alignment", "Deep Learning", "Decision Theory"],
    papers: 115,
    posts: 42,
    hIndex: 49,
    bio: "Researcher at University of Toronto focusing on value alignment and deep learning.",
    fullBio: "Dr. Johnny Brown is a researcher specializing in value alignment, deep learning, decision theory. They have published extensively in top-tier venues and are actively involved in the ea community.",
    website: "https://johnny-brown.com",
    twitter: "johnnybrown",
    publications: [],
    recentPosts: [],
    collaborators: [],
  },
  {
    id: 86,
    slug: "ronald-phillips",
    name: "Dr. Ronald Phillips",
    avatar: "/portraits/imgs11.jpg",
    affiliation: "Anthropic",
    communities: ["academic", "ea"],
    topics: ["Interpretability", "Decision Theory"],
    papers: 131,
    posts: 40,
    hIndex: 18,
    bio: "Researcher at Oxford FHI focusing on interpretability and decision theory.",
    fullBio: "Dr. Ronald Phillips is a researcher specializing in interpretability, decision theory. They have published extensively in top-tier venues and are actively involved in the academic community.",
    website: "https://ronald-phillips.com",
    twitter: "ronaldphillips",
    publications: [],
    recentPosts: [],
    collaborators: [],
  },
  {
    id: 87,
    slug: "emma-butler",
    name: "Dr. Emma Butler",
    avatar: "/portraits/imgs38.jpg",
    affiliation: "University of Toronto",
    communities: ["academic", "lesswrong"],
    topics: ["Decision Theory", "AI Alignment", "Interpretability", "Value Alignment"],
    papers: 135,
    posts: 50,
    hIndex: 49,
    bio: "Researcher at EleutherAI focusing on decision theory and ai alignment.",
    fullBio: "Dr. Emma Butler is a researcher specializing in decision theory, ai alignment, interpretability, value alignment. They have published extensively in top-tier venues and are actively involved in the academic community.",
    website: "https://emma-butler.com",
    twitter: "emmabutler",
    publications: [],
    recentPosts: [],
    collaborators: [],
  },
  {
    id: 88,
    slug: "brian-anderson",
    name: "Dr. Brian Anderson",
    avatar: "/portraits/imgs29.jpg",
    affiliation: "MIRI",
    communities: ["ea"],
    topics: ["AI Alignment", "RLHF"],
    papers: 55,
    posts: 17,
    hIndex: 26,
    bio: "Researcher at Cambridge focusing on ai alignment and rlhf.",
    fullBio: "Dr. Brian Anderson is a researcher specializing in ai alignment, rlhf. They have published extensively in top-tier venues and are actively involved in the ea community.",
    website: "https://brian-anderson.com",
    twitter: "briananderson",
    publications: [],
    recentPosts: [],
    collaborators: [],
  },
  {
    id: 89,
    slug: "nancy-lee",
    name: "Dr. Nancy Lee",
    avatar: "/portraits/imgs32.jpg",
    affiliation: "Google DeepMind",
    communities: ["ea"],
    topics: ["Interpretability", "Decision Theory", "AI Risk", "AI Governance"],
    papers: 101,
    posts: 37,
    hIndex: 37,
    bio: "Researcher at Stanford HAI focusing on interpretability and decision theory.",
    fullBio: "Dr. Nancy Lee is a researcher specializing in interpretability, decision theory, ai risk, ai governance. They have published extensively in top-tier venues and are actively involved in the ea community.",
    website: "https://nancy-lee.com",
    twitter: "nancylee",
    publications: [],
    recentPosts: [],
    collaborators: [],
  },
  {
    id: 90,
    slug: "lawrence-anderson",
    name: "Dr. Lawrence Anderson",
    avatar: "/portraits/imgs20.jpg",
    affiliation: "EleutherAI",
    communities: ["lesswrong", "academic", "ea"],
    topics: ["AI Risk", "Interpretability", "Value Alignment", "Decision Theory"],
    papers: 102,
    posts: 11,
    hIndex: 54,
    bio: "Researcher at Allen Institute for AI focusing on ai risk and interpretability.",
    fullBio: "Dr. Lawrence Anderson is a researcher specializing in ai risk, interpretability, value alignment, decision theory. They have published extensively in top-tier venues and are actively involved in the lesswrong community.",
    website: "https://lawrence-anderson.com",
    twitter: "lawrenceanderson",
    publications: [],
    recentPosts: [],
    collaborators: [],
  },
  {
    id: 91,
    slug: "jacqueline-hayes",
    name: "Dr. Jacqueline Hayes",
    avatar: "/portraits/imgs30.jpg",
    affiliation: "Cambridge",
    communities: ["lesswrong", "academic"],
    topics: ["AI Risk", "Value Alignment", "Interpretability"],
    papers: 46,
    posts: 24,
    hIndex: 48,
    bio: "Researcher at Allen Institute for AI focusing on ai risk and value alignment.",
    fullBio: "Dr. Jacqueline Hayes is a researcher specializing in ai risk, value alignment, interpretability. They have published extensively in top-tier venues and are actively involved in the lesswrong community.",
    website: "https://jacqueline-hayes.com",
    twitter: "jacquelinehayes",
    publications: [],
    recentPosts: [],
    collaborators: [],
  },
  {
    id: 92,
    slug: "jeffrey-phillips",
    name: "Dr. Jeffrey Phillips",
    avatar: "/portraits/imgs20.jpg",
    affiliation: "Oxford FHI",
    communities: ["academic", "lesswrong", "ea"],
    topics: ["Deep Learning", "Mechanistic Interpretability"],
    papers: 146,
    posts: 30,
    hIndex: 53,
    bio: "Researcher at Cambridge focusing on deep learning and mechanistic interpretability.",
    fullBio: "Dr. Jeffrey Phillips is a researcher specializing in deep learning, mechanistic interpretability. They have published extensively in top-tier venues and are actively involved in the academic community.",
    website: "https://jeffrey-phillips.com",
    twitter: "jeffreyphillips",
    publications: [],
    recentPosts: [],
    collaborators: [],
  },
  {
    id: 93,
    slug: "victoria-hill",
    name: "Dr. Victoria Hill",
    avatar: "/portraits/imgs15.jpg",
    affiliation: "MIT CSAIL",
    communities: ["lesswrong", "ea"],
    topics: ["Mechanistic Interpretability", "RLHF", "AI Risk"],
    papers: 87,
    posts: 43,
    hIndex: 54,
    bio: "Researcher at MIRI focusing on mechanistic interpretability and rlhf.",
    fullBio: "Dr. Victoria Hill is a researcher specializing in mechanistic interpretability, rlhf, ai risk. They have published extensively in top-tier venues and are actively involved in the lesswrong community.",
    website: "https://victoria-hill.com",
    twitter: "victoriahill",
    publications: [],
    recentPosts: [],
    collaborators: [],
  },
  {
    id: 94,
    slug: "jacob-adams",
    name: "Dr. Jacob Adams",
    avatar: "/portraits/imgs19.jpg",
    affiliation: "EleutherAI",
    communities: ["academic", "ea"],
    topics: ["RLHF", "AI Alignment", "Interpretability", "Decision Theory"],
    papers: 117,
    posts: 25,
    hIndex: 41,
    bio: "Researcher at UC Berkeley focusing on rlhf and ai alignment.",
    fullBio: "Dr. Jacob Adams is a researcher specializing in rlhf, ai alignment, interpretability, decision theory. They have published extensively in top-tier venues and are actively involved in the academic community.",
    website: "https://jacob-adams.com",
    twitter: "jacobadams",
    publications: [],
    recentPosts: [],
    collaborators: [],
  },
  {
    id: 95,
    slug: "judith-kelly",
    name: "Dr. Judith Kelly",
    avatar: "/portraits/imgs20.jpg",
    affiliation: "OpenAI",
    communities: ["ea"],
    topics: ["AI Governance", "Value Alignment", "Decision Theory", "Deep Learning"],
    papers: 65,
    posts: 33,
    hIndex: 41,
    bio: "Researcher at Carnegie Mellon University focusing on ai governance and value alignment.",
    fullBio: "Dr. Judith Kelly is a researcher specializing in ai governance, value alignment, decision theory, deep learning. They have published extensively in top-tier venues and are actively involved in the ea community.",
    website: "https://judith-kelly.com",
    twitter: "judithkelly",
    publications: [],
    recentPosts: [],
    collaborators: [],
  },
  {
    id: 96,
    slug: "dorothy-hill",
    name: "Dr. Dorothy Hill",
    avatar: "/portraits/imgs11.jpg",
    affiliation: "Carnegie Mellon University",
    communities: ["ea", "lesswrong", "academic"],
    topics: ["RLHF", "AI Alignment"],
    papers: 36,
    posts: 28,
    hIndex: 58,
    bio: "Researcher at EleutherAI focusing on rlhf and ai alignment.",
    fullBio: "Dr. Dorothy Hill is a researcher specializing in rlhf, ai alignment. They have published extensively in top-tier venues and are actively involved in the ea community.",
    website: "https://dorothy-hill.com",
    twitter: "dorothyhill",
    publications: [],
    recentPosts: [],
    collaborators: [],
  },
  {
    id: 97,
    slug: "beverly-garcia",
    name: "Dr. Beverly Garcia",
    avatar: "/portraits/imgs40.jpg",
    affiliation: "University of Toronto",
    communities: ["ea"],
    topics: ["Mechanistic Interpretability", "AI Governance", "Decision Theory", "Value Alignment"],
    papers: 47,
    posts: 8,
    hIndex: 55,
    bio: "Researcher at Cambridge focusing on mechanistic interpretability and ai governance.",
    fullBio: "Dr. Beverly Garcia is a researcher specializing in mechanistic interpretability, ai governance, decision theory, value alignment. They have published extensively in top-tier venues and are actively involved in the ea community.",
    website: "https://beverly-garcia.com",
    twitter: "beverlygarcia",
    publications: [],
    recentPosts: [],
    collaborators: [],
  },
  {
    id: 98,
    slug: "rachel-flores",
    name: "Dr. Rachel Flores",
    avatar: "/portraits/imgs15.jpg",
    affiliation: "Cambridge",
    communities: ["lesswrong"],
    topics: ["AI Risk", "Mechanistic Interpretability", "Interpretability", "RLHF"],
    papers: 43,
    posts: 43,
    hIndex: 39,
    bio: "Researcher at Google DeepMind focusing on ai risk and mechanistic interpretability.",
    fullBio: "Dr. Rachel Flores is a researcher specializing in ai risk, mechanistic interpretability, interpretability, rlhf. They have published extensively in top-tier venues and are actively involved in the lesswrong community.",
    website: "https://rachel-flores.com",
    twitter: "rachelflores",
    publications: [],
    recentPosts: [],
    collaborators: [],
  },
  {
    id: 99,
    slug: "victoria-morgan",
    name: "Dr. Victoria Morgan",
    avatar: "/portraits/imgs12.jpg",
    affiliation: "Oxford FHI",
    communities: ["ea"],
    topics: ["AI Alignment", "AI Governance", "Interpretability", "AI Risk"],
    papers: 58,
    posts: 44,
    hIndex: 37,
    bio: "Researcher at Anthropic focusing on ai alignment and ai governance.",
    fullBio: "Dr. Victoria Morgan is a researcher specializing in ai alignment, ai governance, interpretability, ai risk. They have published extensively in top-tier venues and are actively involved in the ea community.",
    website: "https://victoria-morgan.com",
    twitter: "victoriamorgan",
    publications: [],
    recentPosts: [],
    collaborators: [],
  },
  {
    id: 100,
    slug: "betty-scott",
    name: "Dr. Betty Scott",
    avatar: "/portraits/imgs41.jpg",
    affiliation: "MIRI",
    communities: ["ea"],
    topics: ["Interpretability", "AI Alignment", "AI Governance"],
    papers: 34,
    posts: 30,
    hIndex: 25,
    bio: "Researcher at Anthropic focusing on interpretability and ai alignment.",
    fullBio: "Dr. Betty Scott is a researcher specializing in interpretability, ai alignment, ai governance. They have published extensively in top-tier venues and are actively involved in the ea community.",
    website: "https://betty-scott.com",
    twitter: "bettyscott",
    publications: [],
    recentPosts: [],
    collaborators: [],
  },
  {
    id: 101,
    slug: "bobby-ross",
    name: "Dr. Bobby Ross",
    avatar: "/portraits/imgs25.jpg",
    affiliation: "Anthropic",
    communities: ["ea", "academic", "lesswrong"],
    topics: ["Deep Learning", "Value Alignment", "AI Governance"],
    papers: 70,
    posts: 1,
    hIndex: 43,
    bio: "Researcher at EleutherAI focusing on deep learning and value alignment.",
    fullBio: "Dr. Bobby Ross is a researcher specializing in deep learning, value alignment, ai governance. They have published extensively in top-tier venues and are actively involved in the ea community.",
    website: "https://bobby-ross.com",
    twitter: "bobbyross",
    publications: [],
    recentPosts: [],
    collaborators: [],
  },
  {
    id: 102,
    slug: "samuel-jenkins",
    name: "Dr. Samuel Jenkins",
    avatar: "/portraits/imgs32.jpg",
    affiliation: "Carnegie Mellon University",
    communities: ["lesswrong"],
    topics: ["Decision Theory", "Value Alignment", "RLHF"],
    papers: 104,
    posts: 26,
    hIndex: 18,
    bio: "Researcher at MIRI focusing on decision theory and value alignment.",
    fullBio: "Dr. Samuel Jenkins is a researcher specializing in decision theory, value alignment, rlhf. They have published extensively in top-tier venues and are actively involved in the lesswrong community.",
    website: "https://samuel-jenkins.com",
    twitter: "samueljenkins",
    publications: [],
    recentPosts: [],
    collaborators: [],
  },
  {
    id: 103,
    slug: "julia-james",
    name: "Dr. Julia James",
    avatar: "/portraits/imgs20.jpg",
    affiliation: "Stanford HAI",
    communities: ["academic"],
    topics: ["AI Governance", "AI Risk", "Value Alignment"],
    papers: 95,
    posts: 26,
    hIndex: 23,
    bio: "Researcher at Anthropic focusing on ai governance and ai risk.",
    fullBio: "Dr. Julia James is a researcher specializing in ai governance, ai risk, value alignment. They have published extensively in top-tier venues and are actively involved in the academic community.",
    website: "https://julia-james.com",
    twitter: "juliajames",
    publications: [],
    recentPosts: [],
    collaborators: [],
  },
  {
    id: 104,
    slug: "robert-griffin",
    name: "Dr. Robert Griffin",
    avatar: "/portraits/imgs38.jpg",
    affiliation: "Cambridge",
    communities: ["academic", "lesswrong"],
    topics: ["Mechanistic Interpretability", "Deep Learning"],
    papers: 82,
    posts: 14,
    hIndex: 59,
    bio: "Researcher at Allen Institute for AI focusing on mechanistic interpretability and deep learning.",
    fullBio: "Dr. Robert Griffin is a researcher specializing in mechanistic interpretability, deep learning. They have published extensively in top-tier venues and are actively involved in the academic community.",
    website: "https://robert-griffin.com",
    twitter: "robertgriffin",
    publications: [],
    recentPosts: [],
    collaborators: [],
  },
  {
    id: 105,
    slug: "virginia-johnson",
    name: "Dr. Virginia Johnson",
    avatar: "/portraits/imgs9.jpg",
    affiliation: "EleutherAI",
    communities: ["lesswrong", "ea", "academic"],
    topics: ["Mechanistic Interpretability", "Interpretability", "AI Risk"],
    papers: 89,
    posts: 48,
    hIndex: 57,
    bio: "Researcher at UC Berkeley focusing on mechanistic interpretability and interpretability.",
    fullBio: "Dr. Virginia Johnson is a researcher specializing in mechanistic interpretability, interpretability, ai risk. They have published extensively in top-tier venues and are actively involved in the lesswrong community.",
    website: "https://virginia-johnson.com",
    twitter: "virginiajohnson",
    publications: [],
    recentPosts: [],
    collaborators: [],
  },
  {
    id: 106,
    slug: "joseph-murphy",
    name: "Dr. Joseph Murphy",
    avatar: "/portraits/imgs23.jpg",
    affiliation: "Anthropic",
    communities: ["ea", "academic"],
    topics: ["AI Alignment", "Value Alignment"],
    papers: 85,
    posts: 38,
    hIndex: 18,
    bio: "Researcher at UC Berkeley focusing on ai alignment and value alignment.",
    fullBio: "Dr. Joseph Murphy is a researcher specializing in ai alignment, value alignment. They have published extensively in top-tier venues and are actively involved in the ea community.",
    website: "https://joseph-murphy.com",
    twitter: "josephmurphy",
    publications: [],
    recentPosts: [],
    collaborators: [],
  },
  {
    id: 107,
    slug: "willie-long",
    name: "Dr. Willie Long",
    avatar: "/portraits/imgs39.jpg",
    affiliation: "University of Toronto",
    communities: ["academic"],
    topics: ["Deep Learning", "AI Governance", "RLHF", "Interpretability"],
    papers: 48,
    posts: 34,
    hIndex: 31,
    bio: "Researcher at EleutherAI focusing on deep learning and ai governance.",
    fullBio: "Dr. Willie Long is a researcher specializing in deep learning, ai governance, rlhf, interpretability. They have published extensively in top-tier venues and are actively involved in the academic community.",
    website: "https://willie-long.com",
    twitter: "willielong",
    publications: [],
    recentPosts: [],
    collaborators: [],
  },
  {
    id: 108,
    slug: "linda-james",
    name: "Dr. Linda James",
    avatar: "/portraits/imgs30.jpg",
    affiliation: "Allen Institute for AI",
    communities: ["lesswrong", "academic", "ea"],
    topics: ["Value Alignment", "Interpretability"],
    papers: 113,
    posts: 2,
    hIndex: 6,
    bio: "Researcher at Oxford FHI focusing on value alignment and interpretability.",
    fullBio: "Dr. Linda James is a researcher specializing in value alignment, interpretability. They have published extensively in top-tier venues and are actively involved in the lesswrong community.",
    website: "https://linda-james.com",
    twitter: "lindajames",
    publications: [],
    recentPosts: [],
    collaborators: [],
  },
  {
    id: 109,
    slug: "megan-hill",
    name: "Dr. Megan Hill",
    avatar: "/portraits/imgs11.jpg",
    affiliation: "Anthropic",
    communities: ["ea", "lesswrong", "academic"],
    topics: ["RLHF", "Decision Theory", "AI Alignment"],
    papers: 121,
    posts: 5,
    hIndex: 38,
    bio: "Researcher at EleutherAI focusing on rlhf and decision theory.",
    fullBio: "Dr. Megan Hill is a researcher specializing in rlhf, decision theory, ai alignment. They have published extensively in top-tier venues and are actively involved in the ea community.",
    website: "https://megan-hill.com",
    twitter: "meganhill",
    publications: [],
    recentPosts: [],
    collaborators: [],
  },
  {
    id: 110,
    slug: "karen-cook",
    name: "Dr. Karen Cook",
    avatar: "/portraits/imgs3.jpg",
    affiliation: "University of Toronto",
    communities: ["ea", "academic"],
    topics: ["Deep Learning", "Mechanistic Interpretability"],
    papers: 137,
    posts: 45,
    hIndex: 20,
    bio: "Researcher at Cambridge focusing on deep learning and mechanistic interpretability.",
    fullBio: "Dr. Karen Cook is a researcher specializing in deep learning, mechanistic interpretability. They have published extensively in top-tier venues and are actively involved in the ea community.",
    website: "https://karen-cook.com",
    twitter: "karencook",
    publications: [],
    recentPosts: [],
    collaborators: [],
  },
]
