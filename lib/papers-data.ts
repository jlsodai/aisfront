export interface Paper {
  id: number
  slug: string
  title: string
  abstract: string
  authors: {
    name: string
    slug?: string // links to researcher profile if exists
  }[]
  year: number
  venue: string
  source: "academic" | "lesswrong" | "ea" | "arxiv"
  topics: string[]
  citations: number
  karma?: number // for LW/EA posts
  comments?: number
  url?: string
  pdf?: string
}

export const sourceLabels: Record<string, { label: string; color: string }> = {
  academic: { label: "Academic", color: "bg-blue-500/20 text-blue-400 border-blue-500/30" },
  arxiv: { label: "arXiv", color: "bg-orange-500/20 text-orange-400 border-orange-500/30" },
  lesswrong: { label: "LessWrong", color: "bg-emerald-500/20 text-emerald-400 border-emerald-500/30" },
  ea: { label: "EA Forum", color: "bg-purple-500/20 text-purple-400 border-purple-500/30" },
}

export const papers: Paper[] = [
  {
    id: 1,
    slug: "scaling-monosemanticity",
    title: "Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet",
    abstract:
      "We report a significant advance in understanding the internal representations of large language models by successfully extracting interpretable features from Claude 3 Sonnet using sparse autoencoders. We identify features corresponding to a vast range of concepts, including cities, people, code syntax, and abstract concepts like deception and bias. This work demonstrates that mechanistic interpretability can scale to frontier models.",
    authors: [{ name: "Adly Templeton" }, { name: "Tom Conerly" }, { name: "Jonathan Marcus" }, { name: "Jack Clark" }],
    year: 2024,
    venue: "Anthropic",
    source: "academic",
    topics: ["Mechanistic Interpretability", "Sparse Autoencoders", "Feature Extraction"],
    citations: 234,
    url: "https://anthropic.com/research/scaling-monosemanticity",
  },
  {
    id: 2,
    slug: "constitutional-ai",
    title: "Constitutional AI: Harmlessness from AI Feedback",
    abstract:
      "We present Constitutional AI (CAI), a method for training a harmless AI assistant through self-improvement without any human feedback labels for harms. The method involves two phases: a supervised learning phase using a small number of constitutional principles, and a reinforcement learning phase where the AI critiques and revises its own responses.",
    authors: [{ name: "Yuntao Bai" }, { name: "Saurav Kadavath" }, { name: "Amanda Askell" }, { name: "Jared Kaplan" }],
    year: 2022,
    venue: "arXiv",
    source: "arxiv",
    topics: ["RLHF", "AI Alignment", "Constitutional AI"],
    citations: 1847,
    url: "https://arxiv.org/abs/2212.08073",
  },
  {
    id: 3,
    slug: "agi-ruin-lethalities",
    title: "AGI Ruin: A List of Lethalities",
    abstract:
      "A comprehensive list of reasons why building aligned AGI is extremely difficult and why most approaches are unlikely to work. Covers core difficulties including inner alignment, mesa-optimization, capability generalization vs alignment generalization, and the difficulty of getting multiple chances at solving alignment.",
    authors: [{ name: "Eliezer Yudkowsky", slug: "eliezer-yudkowsky" }],
    year: 2022,
    venue: "LessWrong",
    source: "lesswrong",
    topics: ["AI Risk", "AGI Safety", "Alignment"],
    citations: 156,
    karma: 1567,
    comments: 423,
    url: "https://lesswrong.com/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities",
  },
  {
    id: 4,
    slug: "deep-rl-human-preferences",
    title: "Deep Reinforcement Learning from Human Preferences",
    abstract:
      "We explore how to train deep neural networks to carry out complex behaviors specified by human preferences. We show that human feedback can be used to train agents to perform a wide range of tasks, from simple simulated robotics to playing Atari games, without any explicit reward function.",
    authors: [
      { name: "Paul Christiano", slug: "paul-christiano" },
      { name: "Jan Leike", slug: "jan-leike" },
      { name: "Tom Brown" },
      { name: "Dario Amodei" },
    ],
    year: 2017,
    venue: "NeurIPS",
    source: "academic",
    topics: ["RLHF", "Deep Learning", "Human Feedback"],
    citations: 2847,
    url: "https://arxiv.org/abs/1706.03741",
  },
  {
    id: 5,
    slug: "eliciting-latent-knowledge",
    title: "Eliciting Latent Knowledge: How to Tell If Your Eyes Deceive You",
    abstract:
      "We describe the problem of eliciting latent knowledge (ELK) from a powerful AI system. The core challenge is: given an AI that has learned about the world, how can we extract what it actually knows, even when it might have incentives to deceive us?",
    authors: [{ name: "Paul Christiano", slug: "paul-christiano" }, { name: "Mark Xu" }, { name: "Ajeya Cotra" }],
    year: 2022,
    venue: "ARC",
    source: "academic",
    topics: ["Alignment", "ELK", "AI Safety"],
    citations: 345,
    url: "https://docs.google.com/document/d/1WwsnJQstPq91_Yh-Ch2XRL8H_EpsnjrC1dwZXR37PC8/edit",
  },
  {
    id: 6,
    slug: "most-important-century",
    title: "The Most Important Century",
    abstract:
      "This series argues that we may be living in the most important century of all time for humanity. The basic case is that we're on track to see transformative AI within this century, and this could lead to a radically different future—for better or worse.",
    authors: [{ name: "Holden Karnofsky", slug: "holden-karnofsky" }],
    year: 2021,
    venue: "EA Forum",
    source: "ea",
    topics: ["AI Risk", "Transformative AI", "Longtermism"],
    citations: 89,
    karma: 2134,
    comments: 312,
    url: "https://www.cold-takes.com/most-important-century/",
  },
  {
    id: 7,
    slug: "helm-evaluation",
    title: "Holistic Evaluation of Language Models",
    abstract:
      "We present HELM, a holistic approach to evaluating language models on a diverse set of scenarios and metrics. HELM consists of a taxonomy covering 16 scenarios and 7 types of metrics, along with a standardized infrastructure for running evaluations.",
    authors: [
      { name: "Percy Liang", slug: "percy-liang" },
      { name: "Rishi Bommasani" },
      { name: "Tony Lee" },
      { name: "others" },
    ],
    year: 2022,
    venue: "arXiv",
    source: "arxiv",
    topics: ["Benchmarking", "Evaluation", "Foundation Models"],
    citations: 1234,
    url: "https://arxiv.org/abs/2211.09110",
  },
  {
    id: 8,
    slug: "transformer-circuits",
    title: "A Mathematical Framework for Transformer Circuits",
    abstract:
      "We reverse engineer transformer language models by analyzing them as computational graphs. We develop mathematical frameworks for understanding attention heads, MLPs, and their interactions, enabling precise statements about what computations transformers perform.",
    authors: [
      { name: "Neel Nanda", slug: "neel-nanda" },
      { name: "Sarah Chen", slug: "sarah-chen" },
      { name: "Chris Olah" },
    ],
    year: 2022,
    venue: "Anthropic",
    source: "academic",
    topics: ["Mechanistic Interpretability", "Transformers", "Circuits"],
    citations: 567,
    url: "https://transformer-circuits.pub/2022/framework/index.html",
  },
  {
    id: 9,
    slug: "specification-gaming",
    title: "Specification Gaming: The Flip Side of AI Ingenuity",
    abstract:
      "We present a comprehensive survey of specification gaming in AI systems—cases where AI systems satisfy the literal specification of their objective while violating its intended spirit. This highlights the difficulty of specifying objectives that capture what we really want.",
    authors: [
      { name: "Victoria Krakovna", slug: "victoria-krakovna" },
      { name: "Jonathan Uesato" },
      { name: "others" },
    ],
    year: 2020,
    venue: "DeepMind Blog",
    source: "academic",
    topics: ["Specification Gaming", "AI Safety", "Reward Hacking"],
    citations: 234,
    url: "https://deepmind.com/blog/specification-gaming",
  },
  {
    id: 10,
    slug: "concrete-problems-ai-safety",
    title: "Concrete Problems in AI Safety",
    abstract:
      "We discuss five practical research problems related to accident risk in machine learning systems: safe exploration, robustness to distributional shift, avoiding negative side effects, avoiding reward hacking, and scalable oversight. These problems are relevant to current ML systems and will become more important as systems become more capable.",
    authors: [
      { name: "Dario Amodei" },
      { name: "Chris Olah" },
      { name: "Jacob Steinhardt" },
      { name: "Paul Christiano", slug: "paul-christiano" },
    ],
    year: 2016,
    venue: "arXiv",
    source: "arxiv",
    topics: ["AI Safety", "Robustness", "Safe Exploration"],
    citations: 3456,
    url: "https://arxiv.org/abs/1606.06565",
  },
  {
    id: 11,
    slug: "200-problems-mech-interp",
    title: "200 Concrete Open Problems in Mechanistic Interpretability",
    abstract:
      "A comprehensive list of 200 concrete, actionable research problems in mechanistic interpretability. Organized by difficulty level and area, covering topics from basic feature finding to ambitious projects like fully reverse-engineering GPT-2.",
    authors: [{ name: "Neel Nanda", slug: "neel-nanda" }],
    year: 2024,
    venue: "LessWrong",
    source: "lesswrong",
    topics: ["Mechanistic Interpretability", "Research Agenda", "Open Problems"],
    citations: 45,
    karma: 1123,
    comments: 145,
    url: "https://www.lesswrong.com/posts/LbrPTJ4fmABEdEnLf/200-concrete-open-problems-in-mechanistic-interpretability",
  },
  {
    id: 12,
    slug: "human-compatible",
    title: "Human Compatible: Artificial Intelligence and the Problem of Control",
    abstract:
      "This book examines the problem of controlling advanced AI systems. It proposes a new foundation for AI development based on three principles: the machine's purpose is to maximize human preferences, the machine is uncertain about what those preferences are, and the source of information about human preferences is human behavior.",
    authors: [{ name: "Stuart Russell", slug: "stuart-russell" }],
    year: 2019,
    venue: "Viking",
    source: "academic",
    topics: ["Value Alignment", "AI Control", "Beneficial AI"],
    citations: 1234,
    url: "https://www.penguinrandomhouse.com/books/566677/human-compatible-by-stuart-russell/",
  },
  {
    id: 13,
    slug: "ai-researcher-survey-2024",
    title: "2024 Expert Survey on Progress in AI",
    abstract:
      "Results from a large-scale survey of AI researchers on timelines to various AI capabilities, expected impacts, and safety concerns. Updates previous surveys with new questions about large language models and recent progress in AI capabilities.",
    authors: [{ name: "Katja Grace", slug: "katja-grace" }, { name: "John Salvatier" }, { name: "others" }],
    year: 2024,
    venue: "AI Impacts",
    source: "ea",
    topics: ["AI Forecasting", "Timelines", "Expert Surveys"],
    citations: 78,
    karma: 934,
    comments: 123,
    url: "https://aiimpacts.org/2024-expert-survey-on-progress-in-ai/",
  },
  {
    id: 14,
    slug: "grokking-mechanistic",
    title: "Progress Measures for Grokking via Mechanistic Interpretability",
    abstract:
      "We study grokking—where neural networks suddenly generalize long after memorizing training data—through the lens of mechanistic interpretability. We find that grokking is explained by the gradual amplification of a generalizing circuit alongside the decay of a memorizing circuit.",
    authors: [{ name: "Neel Nanda", slug: "neel-nanda" }, { name: "Lawrence Chan" }, { name: "Tom Lieberum" }],
    year: 2023,
    venue: "ICLR",
    source: "academic",
    topics: ["Mechanistic Interpretability", "Grokking", "Phase Transitions"],
    citations: 234,
    url: "https://arxiv.org/abs/2301.05217",
  },
  {
    id: 15,
    slug: "managing-ai-risks",
    title: "Managing AI Risks in an Era of Rapid Progress",
    abstract:
      "A joint statement from leading AI researchers calling for urgent governance measures to manage the risks from advanced AI systems. Proposes concrete policy recommendations including mandatory safety evaluations, international coordination, and public funding for AI safety research.",
    authors: [
      { name: "Yoshua Bengio", slug: "yoshua-bengio" },
      { name: "Stuart Russell", slug: "stuart-russell" },
      { name: "Geoffrey Hinton" },
      { name: "others" },
    ],
    year: 2024,
    venue: "Science",
    source: "academic",
    topics: ["AI Governance", "AI Risk", "Policy"],
    citations: 89,
    url: "https://www.science.org/doi/10.1126/science.adn0117",
  },
  {
    id: 16,
    slug: "alignment-problem-deep-learning",
    title: "The Alignment Problem from a Deep Learning Perspective",
    abstract:
      "An analysis of why aligning powerful AI systems might be difficult, written from the perspective of deep learning research. Covers deceptive alignment, emergent goals, situational awareness, and potential solutions like interpretability and oversight.",
    authors: [{ name: "Richard Ngo", slug: "richard-ngo" }, { name: "Lawrence Chan" }, { name: "Sören Mindermann" }],
    year: 2023,
    venue: "arXiv",
    source: "arxiv",
    topics: ["Alignment", "Deep Learning", "AGI Safety"],
    citations: 189,
    url: "https://arxiv.org/abs/2209.00626",
  },
  {
    id: 17,
    slug: "weak-to-strong-generalization",
    title: "Weak-to-Strong Generalization: Eliciting Strong Capabilities with Weak Supervision",
    abstract:
      "We study an analogy for aligning future superhuman models: can weak models supervise strong models? We find that when strong models are finetuned on labels from weak supervisors, they can generalize beyond their supervisors—pointing to possibilities for scalable oversight.",
    authors: [{ name: "Collin Burns" }, { name: "Haotian Ye" }, { name: "Dan Klein" }, { name: "Jacob Steinhardt" }],
    year: 2023,
    venue: "arXiv",
    source: "arxiv",
    topics: ["Scalable Oversight", "Superalignment", "Weak-to-Strong"],
    citations: 456,
    url: "https://arxiv.org/abs/2312.09390",
  },
  {
    id: 18,
    slug: "sleeper-agents",
    title: "Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training",
    abstract:
      "We demonstrate that current safety training techniques do not reliably remove backdoors from large language models. Models trained to behave maliciously in specific contexts maintain these behaviors even after RLHF and adversarial training.",
    authors: [
      { name: "Evan Hubinger" },
      { name: "Carson Denison" },
      { name: "Jesse Mu" },
      { name: "Monte MacDiarmid" },
    ],
    year: 2024,
    venue: "arXiv",
    source: "arxiv",
    topics: ["Deceptive Alignment", "Safety Training", "Backdoors"],
    citations: 178,
    url: "https://arxiv.org/abs/2401.05566",
  },
  {
    id: 19,
    slug: "superalignment-fast-takeoff",
    title: "What Would It Take to Align a Superintelligence?",
    abstract:
      "An exploration of the challenges and potential approaches to aligning AI systems significantly smarter than humans. Discusses why current alignment techniques may fail to scale and what new approaches might be needed.",
    authors: [{ name: "Jan Leike", slug: "jan-leike" }, { name: "David Krueger" }, { name: "others" }],
    year: 2024,
    venue: "LessWrong",
    source: "lesswrong",
    topics: ["Superalignment", "AGI Safety", "Scalable Oversight"],
    citations: 34,
    karma: 567,
    comments: 89,
    url: "https://lesswrong.com/posts/example",
  },
  {
    id: 20,
    slug: "steering-vectors",
    title: "Representation Engineering: A Top-Down Approach to AI Safety",
    abstract:
      "We introduce representation engineering, which works with high-level representations rather than model weights. We show that simple steering vectors can control model behavior, enabling safer and more interpretable AI systems.",
    authors: [
      { name: "Andy Zou" },
      { name: "Long Phan" },
      { name: "Sarah Chen", slug: "sarah-chen" },
      { name: "others" },
    ],
    year: 2023,
    venue: "arXiv",
    source: "arxiv",
    topics: ["Representation Engineering", "Steering Vectors", "Interpretability"],
    citations: 312,
    url: "https://arxiv.org/abs/2310.01405",
  },
]

export const allTopics = [
  "AI Alignment",
  "AI Control",
  "AI Forecasting",
  "AI Governance",
  "AI Risk",
  "AI Safety",
  "AGI Safety",
  "Backdoors",
  "Benchmarking",
  "Beneficial AI",
  "Circuits",
  "Constitutional AI",
  "Deceptive Alignment",
  "Deep Learning",
  "ELK",
  "Evaluation",
  "Expert Surveys",
  "Feature Extraction",
  "Foundation Models",
  "Grokking",
  "Human Feedback",
  "Interpretability",
  "Longtermism",
  "Mechanistic Interpretability",
  "Open Problems",
  "Phase Transitions",
  "Policy",
  "Representation Engineering",
  "Research Agenda",
  "Reward Hacking",
  "RLHF",
  "Robustness",
  "Safe Exploration",
  "Safety Training",
  "Scalable Oversight",
  "Sparse Autoencoders",
  "Specification Gaming",
  "Steering Vectors",
  "Superalignment",
  "Timelines",
  "Transformative AI",
  "Transformers",
  "Value Alignment",
  "Weak-to-Strong",
]
